{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67b7cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\python310\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python310\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python310\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473484b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21aded87",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd507a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13225, 5922, 13]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hello World.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eaa6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28519cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fce483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tockenizer:\n",
    "    def __init__(self,model=\"o200k_base\"):\n",
    "        self.encoder = tiktoken.get_encoding(model)\n",
    "        self.n_vocab = self.encoder.n_vocab\n",
    "        \n",
    "    def encode(self,sentence:str):\n",
    "        return self.encoder.encode(sentence)\n",
    "    \n",
    "    def decode(self,tok):\n",
    "        return self.encoder.decode(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e1ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "    \"Hello, how are you?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "tokenizer = Tockenizer('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00e2bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15496, 11, 703, 389, 345, 30],\n",
       " [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(inputs[0]),tokenizer.encode(inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de15924",
   "metadata": {},
   "source": [
    "## Data Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0af79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = tokenizer.encode(inputs[1])\n",
    "enc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b62aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [464, 2068, 7586, 21831]\n",
      "y:\t [2068, 7586, 21831, 18045]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(\"x:\",x)\n",
    "print(\"y:\\t\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3794bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87b0cd",
   "metadata": {},
   "source": [
    "### LLM Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d675bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            inp_chk = token_ids[i: i+max_length]\n",
    "            target_chk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.Tensor(inp_chk).to(dtype=torch.int32))\n",
    "            self.target_ids.append(torch.Tensor(target_chk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc64543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "        Narendra Damodardas Modi (born 17 September 1950) is an Indian politician who has served as the prime minister of India since 2014.\n",
    "        Modi was the chief minister of Gujarat from 2001 to 2014 and is the member of parliament (MP) for Varanasi. He is a member of the Bharatiya \n",
    "        Janata Party (BJP) and of the Rashtriya Swayamsevak Sangh (RSS), a far-right Hindu nationalist paramilitary volunteer organisation. He is the longest-serving prime minister outside the Indian National Congress.\n",
    "        Modi was born and raised in Vadnagar in northeastern Gujarat, where he completed his secondary education. He was introduced to the RSS at the age of eight. At the age of 18, he was married to Jashodaben Modi, \n",
    "        whom he abandoned soon after, only publicly acknowledging her four decades later when legally required to do so. Modi became a full-time worker for the RSS in Gujarat in 1971.\n",
    "        The RSS assigned him to the BJP in 1985 and he rose through the party hierarchy, becoming general secretary in 1998. In 2001, Modi was appointed chief minister of Gujarat and elected to the legislative assembly soon after. \n",
    "        His administration is considered complicit in the 2002 Gujarat riots,[d] and has been criticised for its management of the crisis. According to official records, a little over 1,000 people were killed, three-quarters of whom were Muslim; \n",
    "        independent sources estimated 2,000 deaths, mostly Muslim.[13] A Special Investigation Team appointed by the Supreme Court of India in 2012 found no evidence to initiate prosecution proceedings against him.\n",
    "        While his policies as chief minister were credited for encouraging economic growth, his administration was criticised for failing to significantly improve health, poverty and education indices in the state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "690e73c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ca36535",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = LLMDataset(text=text, tokenizer=tokenizer, max_length=6,stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f11e6028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07e796ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([198, 220, 220, 220, 220, 220], dtype=torch.int32)\n",
      "target:\t tensor([220., 220., 220., 220., 220., 220.])\n"
     ]
    }
   ],
   "source": [
    "print(\"input:\",dataset[0][0])\n",
    "print(\"target:\\t\",dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fd7c4",
   "metadata": {},
   "source": [
    "### Create Data Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18da198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(text, tokenizer, batch_size=64, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = LLMDataset(text=text, tokenizer=tokenizer, max_length=6,stride=2)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27026786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[  198,   220,   220,   220,   220,   220],\n",
      "        [  220,   220,   220,   220,   220,   220],\n",
      "        [  220,   220,   220,   220, 28113,  5245],\n",
      "        [  220,   220, 28113,  5245,   375,   446]], dtype=torch.int32)\n",
      "Targets:\n",
      "tensor([[  220.,   220.,   220.,   220.,   220.,   220.],\n",
      "        [  220.,   220.,   220.,   220.,   220., 28113.],\n",
      "        [  220.,   220.,   220., 28113.,  5245.,   375.],\n",
      "        [  220., 28113.,  5245.,   375.,   446.,   292.]])\n"
     ]
    }
   ],
   "source": [
    "max_length = 6\n",
    "dataloader = create_data_loader(text=text,tokenizer=tokenizer,max_length=max_length,stride=2,shuffle=False,batch_size=4)\n",
    "\n",
    "data_itr = iter(dataloader)\n",
    "first_batch = next(data_itr)\n",
    "inputs, targets = first_batch\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "print(\"Targets:\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf59592",
   "metadata": {},
   "source": [
    "## Embedding Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "265dbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "978368a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 256])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "170c5a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35cf81e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 256])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embedding_layer(inputs)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df93bb",
   "metadata": {},
   "source": [
    "### Embeddings with positional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1618a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "249bac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d41ae83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[  198,   220,   220,   220,   220,   220],\n",
      "        [  220,   220,   220,   220,   220,   220],\n",
      "        [  220,   220,   220,   220, 28113,  5245],\n",
      "        [  220,   220, 28113,  5245,   375,   446]], dtype=torch.int32)\n",
      "Inputs shape:\n",
      "torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "max_length = 6\n",
    "\n",
    "\n",
    "dataloader = create_data_loader(text=text,tokenizer=tokenizer,max_length=max_length,stride=2,shuffle=False,batch_size=4)\n",
    "\n",
    "data_itr = iter(dataloader)\n",
    "first_batch = next(data_itr)\n",
    "inputs, targets = first_batch\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "print(\"Inputs shape:\")\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "093e5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b54bcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa9a4940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 256])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c7d8d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4086, -0.9019, -0.8384,  ...,  1.0736,  1.6007, -1.4017],\n",
      "        [ 0.4155,  1.6523,  0.0712,  ..., -0.5711, -1.4070, -0.5012],\n",
      "        [ 0.6352, -1.5864, -1.2704,  ..., -0.6178, -0.4244,  0.1902],\n",
      "        [-1.7419, -1.8249,  0.6606,  ..., -0.4548, -0.2138, -0.4476],\n",
      "        [-0.2468,  0.2067,  0.0863,  ...,  1.8337, -0.7942, -0.0705],\n",
      "        [-1.8092, -1.4747, -1.0397,  ...,  1.8867, -0.0478,  1.1137]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "positional_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(positional_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97d93b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings+positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7eca43cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 256])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb3fb36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5532, -0.2105, -1.1234,  ...,  0.5953, -0.1279, -1.4442],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439]],\n",
       "\n",
       "        [[ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439]],\n",
       "\n",
       "        [[ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [-0.2619, -0.5330, -0.4559,  ...,  1.9033,  0.7191,  0.5654],\n",
       "         [ 0.4012,  0.4216,  0.8718,  ...,  0.3147, -1.8883, -0.8934]],\n",
       "\n",
       "        [[ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [ 1.9156, -1.1700,  1.8956,  ..., -1.0003, -0.2094,  1.2439],\n",
       "         [-0.2619, -0.5330, -0.4559,  ...,  1.9033,  0.7191,  0.5654],\n",
       "         [ 0.4012,  0.4216,  0.8718,  ...,  0.3147, -1.8883, -0.8934],\n",
       "         [ 0.6198,  0.8013, -0.9236,  ..., -0.4286,  0.4014,  0.4015],\n",
       "         [ 2.1666,  0.0839,  0.6546,  ...,  0.0239,  0.5801,  0.0188]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2605f508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4086, -0.9019, -0.8384,  ...,  1.0736,  1.6007, -1.4017],\n",
       "        [ 0.4155,  1.6523,  0.0712,  ..., -0.5711, -1.4070, -0.5012],\n",
       "        [ 0.6352, -1.5864, -1.2704,  ..., -0.6178, -0.4244,  0.1902],\n",
       "        [-1.7419, -1.8249,  0.6606,  ..., -0.4548, -0.2138, -0.4476],\n",
       "        [-0.2468,  0.2067,  0.0863,  ...,  1.8337, -0.7942, -0.0705],\n",
       "        [-1.8092, -1.4747, -1.0397,  ...,  1.8867, -0.0478,  1.1137]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc2327ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9618, -1.1125, -1.9619,  ...,  1.6689,  1.4728, -2.8459],\n",
       "         [ 2.3311,  0.4823,  1.9668,  ..., -1.5714, -1.6164,  0.7428],\n",
       "         [ 2.5507, -2.7564,  0.6252,  ..., -1.6181, -0.6338,  1.4341],\n",
       "         [ 0.1736, -2.9949,  2.5562,  ..., -1.4551, -0.4231,  0.7963],\n",
       "         [ 1.6687, -0.9633,  1.9819,  ...,  0.8334, -1.0036,  1.1734],\n",
       "         [ 0.1064, -2.6446,  0.8559,  ...,  0.8864, -0.2571,  2.3576]],\n",
       "\n",
       "        [[ 3.3241, -2.0719,  1.0572,  ...,  0.0733,  1.3913, -0.1578],\n",
       "         [ 2.3311,  0.4823,  1.9668,  ..., -1.5714, -1.6164,  0.7428],\n",
       "         [ 2.5507, -2.7564,  0.6252,  ..., -1.6181, -0.6338,  1.4341],\n",
       "         [ 0.1736, -2.9949,  2.5562,  ..., -1.4551, -0.4231,  0.7963],\n",
       "         [ 1.6687, -0.9633,  1.9819,  ...,  0.8334, -1.0036,  1.1734],\n",
       "         [ 0.1064, -2.6446,  0.8559,  ...,  0.8864, -0.2571,  2.3576]],\n",
       "\n",
       "        [[ 3.3241, -2.0719,  1.0572,  ...,  0.0733,  1.3913, -0.1578],\n",
       "         [ 2.3311,  0.4823,  1.9668,  ..., -1.5714, -1.6164,  0.7428],\n",
       "         [ 2.5507, -2.7564,  0.6252,  ..., -1.6181, -0.6338,  1.4341],\n",
       "         [ 0.1736, -2.9949,  2.5562,  ..., -1.4551, -0.4231,  0.7963],\n",
       "         [-0.5087, -0.3263, -0.3696,  ...,  3.7370, -0.0752,  0.4948],\n",
       "         [-1.4080, -1.0530, -0.1679,  ...,  2.2014, -1.9361,  0.2203]],\n",
       "\n",
       "        [[ 3.3241, -2.0719,  1.0572,  ...,  0.0733,  1.3913, -0.1578],\n",
       "         [ 2.3311,  0.4823,  1.9668,  ..., -1.5714, -1.6164,  0.7428],\n",
       "         [ 0.3733, -2.1194, -1.7263,  ...,  1.2856,  0.2947,  0.7556],\n",
       "         [-1.3407, -1.4032,  1.5324,  ..., -0.1402, -2.1021, -1.3410],\n",
       "         [ 0.3730,  1.0080, -0.8373,  ...,  1.4051, -0.3928,  0.3310],\n",
       "         [ 0.3574, -1.3908, -0.3851,  ...,  1.9106,  0.5324,  1.1325]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a51986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
